{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf40d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.csgraph import *\n",
    "from scipy.sparse import csr_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090ea9c",
   "metadata": {},
   "source": [
    "# Introduction: Graphs\n",
    "\n",
    "A [graph](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) $G = (V, E)$ is a mathematical construct consisting of both\n",
    "- A set $V$ of vertices (read: nodes), and \n",
    "- A set $E$ of edges (read: connections) consisting of pairs of vertices from $V$.\n",
    "\n",
    "In fact, we have already seen a type of graph in the form of [trees](https://en.wikipedia.org/wiki/Tree_(graph_theory)).  \n",
    "Trees happen to be undirected acyclic graphs (terms defined below).  \n",
    "A representative problem in graph theory is the [seven bridges of Konigsberg](https://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg).  \n",
    "> **Given:** A map of Konigsberg from Wikipedia.  \n",
    "**Problem:** Is there a path that crosses all bridges exactly once?  \n",
    "<img src=\"img/09-10_kberg.png\" style=\"width: 30em\" />  \n",
    "> **Answer:** No (from Euler).   \n",
    "**Reason:** Ignore all the objects (the buildings, the water, the land, etc.) and details (the shapes and distances) in the image.  \n",
    "The only relevant information is the fact that there are 4 islands with a number of connections between them.  \n",
    "This reduces the map to the drawing on the right, where islands are abstracted as vertices and bridges are edges between them.  \n",
    "<img src=\"img/09-10_7bk.png\" style=\"width: 50em\" />  \n",
    "Suppose there *were* a hypothetical path to cross each bridge exactly once.  \n",
    "Then aside from the start and end, every vertex has to have an even number of edges (one for entry and one for exit).  \n",
    "In other words, at most 2 vertices can have an odd number of edges to it.  \n",
    "Since all 4 vertices have an odd number of edges, such a path is not possible on this configuration.  \n",
    "\n",
    "Fundamentally, graphs are reductive/invariant representations of *networks,* and the study of graphs is integral to network science.   \n",
    "Here are some networks you may be familiar with, and where graph theory has been applied: \n",
    "- Social networks\n",
    "- Telecom\n",
    "- Supply chains\n",
    "- Natural language/linguistics\n",
    "\n",
    "### Flavors of Graphs\n",
    "\n",
    "Given graph $G = (V, E)$, some possible descriptors of it are: \n",
    "- **Directed/Undirected:**  \n",
    "In *undirected* graphs, if edge $(x, y) \\in E$, then the edge in the opposite direction $(y, x) \\in E$ as well.  \n",
    "*Directed* graphs have no such restriction.  \n",
    "- **Weighted/Unweighted:**  \n",
    "In *weighted* graphs, a number (read: weight) can be assigned to edges between vertices.  \n",
    "Such a number can refer to any number of properties, including distance and association strength.  \n",
    "We will explore algorithms on weighted graphs in the next session.  \n",
    "For now, we will consider *unweighted* graphs, where we only know the connectivity of the vertices.  \n",
    "- **Simple/Non-simple:**  \n",
    "*Self-edges* are edges which start and end at the same vertex.  \n",
    "*Multi-edges* are edge which occur more than once in a graph.  \n",
    "*Simple* graphs contain neither self-edges nor multi-edges.  \n",
    "- **Dense/Sparse:**  \n",
    "A graph is either *dense* or *sparse* depending on the number of edges relative to the number of vertices.  \n",
    "There is no formal definition that delineates the two, but roughly, one expects a sparse graph of $N$ vertices to have $\\propto N$ edges.  \n",
    "Dense graphs would have somewhere around $N^2$ or more edges.  \n",
    "- **Cyclic/Acyclic:**  \n",
    "A graph is *cyclic* if it contains a cycle, and *acyclic* otherwise.  \n",
    "A *cycle* is a path (read: directed sequence of edges) that starts and ends at same vertex.  \n",
    "An important graph for scheduling applications is the *directed acyclic graphs (DAGs)*.\n",
    "- **Embedded/Topological:**  \n",
    "*Embedded* graphs are those whose vertices and edges have geometric positions assigned to them.  \n",
    "Topological graphs are independent of geometry (see: Konigsberg graph above).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36884b20",
   "metadata": {},
   "source": [
    "# Representations of Graphs\n",
    "\n",
    "Euler's problem with his Konigsberg solution was that he had no formal theory behind it (to be fair, he was inventing it).  \n",
    "It's one thing to draw pictures, but it's mostly useless if we can't express it with math and numbers.  \n",
    "From our perspective, we would at least have trouble getting it into the computer for our algorithms.  \n",
    "Fortunately, the mathematics and computer science has been worked out, and much of it should be familiar to you from linear algebra and hashing.  \n",
    "There are 2 general representations of graphs: **adjacency lists** and **adjacency matrices**.  \n",
    "\n",
    "Before proceeding, I will make one clarification.  \n",
    "The book claims that between lists and matrices, the former are more useful for most graph problems.  \n",
    "This is true in general because most graph porblems you would try to handle are sparse, and lists are space efficient.  \n",
    "This *would* be true for us if we could directly manage memory and make generic linked lists easily.  \n",
    "As it stands, your coursework and Python itself are better suited to using arrays and matrices, so that will be our focus.  \n",
    "\n",
    "## Adjacency Lists\n",
    "Suppose a graph $G = (V, E)$ contains $N$ vertices and $M$ edges.  \n",
    "We can represent it with an $N$-length array `L` of pointers to linked lists, where \n",
    "1. Each index $i$ corresponds to a vertex. \n",
    "1. Element $L_i$ points to/contains a linked list of neighbors of vertex $i$. \n",
    "\n",
    "You may recognize the similarity to chaining from hashing and hash-collisions.  \n",
    "It carries many of the advantages (low space requirments, conceptually simple, fast linked list operations).  \n",
    "One disadvantage for graph analysis is that we may occasionally want to do linear algebra on the graphs, and this structure does not easily support this. \n",
    "\n",
    "## Adjacency Matrices \n",
    "Suppose a graph $G = (V, E)$ contains $N$ vertices and $M$ edges.  \n",
    "We can represent it with an $N\\times N$ adjacency matrix `M` , where \n",
    "1. Each row/column i corresponds to a vertex\n",
    "1. Element $M_{ij}$ is either $1$ if $(i, j)$ is an edge of $G$, else $0$. \n",
    "\n",
    "Mathematically, this representation is far more convenient/amenable to theory-work via [graph polynomials](https://en.wikipedia.org/wiki/Graph_polynomial).  \n",
    "Among other things, $M^k$ tells you the number of $k$-length paths between any two vertices.  \n",
    "Many of the properties of the graph also translate to the matrix, e.g. \n",
    "- Undirected $\\rightarrow$ symmetric. \n",
    "- Simple $\\rightarrow$ boolean and $\\forall i \\quad M_{ii} = 0$ \n",
    "- Cyclic $\\rightarrow \\exists k>1$ such that $Tr(M^k) \\neq 0$ \n",
    "- Sparse $\\rightarrow$ sparse.  \n",
    "- DAG $\\rightarrow $ nilpotent. \n",
    "\n",
    "Unfortunately a direct implementation of the matrix is also incredibly space-inefficient for sparse graphs since the matrix size grows as $N^2$.  \n",
    "\n",
    "## COO & CSR Format\n",
    "If we know the graph/matrix is sparse, then we can reduce the size it takes up in memory.  \n",
    "Given an $M \\times N$ matrix, a *coordinate list (COO)* format stores all $X$ non-zero elements as 3 arrays:\n",
    "- `V`: Non-zero values (array length $X$) \n",
    "- `C`: Column index of values (array length $X$, values in $[0, N-1]$). \n",
    "- `R`: Row index of values (array length $X$, values in $[0, M-1]$). \n",
    "\n",
    "Since $X \\approx N$ in sparse graphs, this representation grows as $N$.  \n",
    "However, this representation is also slow at arithmetic/linear algebra (think about why).  \n",
    "\n",
    "An alternative is the *compressed sparse row (CSR)* format. \n",
    "Given an $M \\times N$ matrix, CSR stores all $X$ non-zero elements as 3 arrays:\n",
    "- `V`: Non-zero values (array length X) \n",
    "- `C`: Column index of values (array length X, values in $[0, N-1]$). \n",
    "- `R`: Starting and ending indices of V and C per row (array length $M+1$, values in $[0, X]$)\n",
    "\n",
    "This still grows as $N$ for sparse matrices, but now it supports fast arithmetic/linear algebra (again, think about why, especially compared to COO). \n",
    "\n",
    "**Note**: There is also a column-equivalent version *(CSC)* which is almost entirely analogus to CSR.  \n",
    "## Python Graph Representations\n",
    "\n",
    "Graph problems are as old as the hills.  \n",
    "So even if Python is not great at making linked structures out of the box, many have already made libraries specifically for graphs.  \n",
    "Some notable libraries and their implementations:\n",
    "- **Scipy:** sparse matrices\n",
    "- **NetworkX:** linked lists\n",
    "- **Pytorch Geometric:** (effectively) dictionaries with tensors\n",
    "\n",
    "In your own time, go through these libraries and make sure you understand why they are implemented the way they are.  \n",
    "For this course I will go with [scipy](https://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html) because: \n",
    "1. The documentation and source code are very approachable (and I will insist, **READ THE DOCUMENTATION**). \n",
    "1. The implementation already supports a lot of the algorithms we want to do. \n",
    "1. I want to make sure you have it installed.\n",
    "\n",
    "In the demo code below, I show the CSR representation of a matrix in scipy using both `csr_array` and `csgraph`.  \n",
    "Either would work for representing a graph, though one distinction is in how they interpret multiplication.  \n",
    "For both `numpy` arrays and `csr_array`, `*` performs element-wise multiplication while `@` performs matrix multplication (dot products of rows and columns).  \n",
    "For `csgraphs`, both `*` and `@` are intepreted as matrix multiplication (presumably for use in graph polynomials).  \n",
    "**If you want to avoid issues related to this**, be specific and use `@` wherever you intend matrix multiplication.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3620fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix (dense):\n",
      "[[0 2 1]\n",
      " [2 0 0]\n",
      " [1 0 0]] \n",
      "\n",
      "csr_array: \n",
      "[2 1 2 1]\n",
      "[1 2 0 0]\n",
      "[0 2 3 4] \n",
      "\n",
      "csgraph:\n",
      "[2. 1. 2. 1.]\n",
      "[1 2 0 0]\n",
      "[0 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Demo code\n",
    "\n",
    "G_dense = np.array([[0, 2, 1],\n",
    "                    [2, 0, 0],\n",
    "                    [1, 0, 0]])\n",
    "\n",
    "print('Matrix (dense):')\n",
    "print(G_dense, '\\n')\n",
    "\n",
    "G_sparse = csr_array(G_dense)\n",
    "print('csr_array: ')\n",
    "print(G_sparse.data)\n",
    "print(G_sparse.indices)\n",
    "print(G_sparse.indptr, '\\n')\n",
    "\n",
    "G2_sparse = csgraph_from_dense(G_dense, null_value=0)\n",
    "print('csgraph:')\n",
    "print(G2_sparse.data)\n",
    "print(G2_sparse.indices)\n",
    "print(G2_sparse.indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6242127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"*\" products \n",
      "\n",
      "Matrix (dense):\n",
      "[[0 4 1]\n",
      " [4 0 0]\n",
      " [1 0 0]]\n",
      "csr_array: \n",
      "  (0, 1)\t4\n",
      "  (0, 2)\t1\n",
      "  (1, 0)\t4\n",
      "  (2, 0)\t1\n",
      "csgraph:\n",
      "  (0, 0)\t5.0\n",
      "  (1, 2)\t2.0\n",
      "  (1, 1)\t4.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 1)\t2.0\n"
     ]
    }
   ],
   "source": [
    "# Demo of sparse products under \"*\"\n",
    "# The first two will be element-wise products, while the last is matrix multiplication\n",
    "print('\"*\" products \\n')\n",
    "print('Matrix (dense):')\n",
    "print(G_dense*G_dense)\n",
    "print('csr_array: ')\n",
    "print(G_sparse*G_sparse)\n",
    "print('csgraph:')\n",
    "print((G2_sparse*G2_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc70381-cb06-4e30-876e-1b1cf4c5435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"@\" products \n",
      "\n",
      "Matrix (dense): \n",
      "[[5 0 0]\n",
      " [0 4 2]\n",
      " [0 2 1]]\n",
      "csr_array: \n",
      "  (0, 0)\t5\n",
      "  (1, 2)\t2\n",
      "  (1, 1)\t4\n",
      "  (2, 2)\t1\n",
      "  (2, 1)\t2\n",
      "csgraph: \n",
      "  (0, 0)\t5.0\n",
      "  (1, 2)\t2.0\n",
      "  (1, 1)\t4.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 1)\t2.0\n"
     ]
    }
   ],
   "source": [
    "# Demo of sparse products under \"@\"\n",
    "# All of them will be matrix multiplication\n",
    "print('\"@\" products \\n')\n",
    "print('Matrix (dense): ')\n",
    "print(G_dense@G_dense)\n",
    "print('csr_array: ')\n",
    "print(G_sparse@G_sparse)\n",
    "print('csgraph: ')\n",
    "print(G2_sparse@G2_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f12a2f",
   "metadata": {},
   "source": [
    "# Graph Traversal \n",
    "\n",
    "One of the most fundamental graph problems is to traverse every edge and vertex.  \n",
    "This can be interpreted most naturally as a maze-solving problem, where:\n",
    "1. Each vertex denotes a junction or fork in the path.  \n",
    "1. Each edge denotes a hallway of the maze.\n",
    "   \n",
    "<img src=\"img/09-10_maze.png\" style=\"width: 30em\" />   \n",
    "For efficiency, we ideally visit each edge at most twice (i.e. backtrack at most once).  \n",
    "\n",
    "For correctness, the traversal must be systematic; we should *know* all the paths to explore.\n",
    "\n",
    "The key idea behind graph traversal is to mark each vertex based on its exploration state.  \n",
    "This can be done most naturally with either Boolean flags or a number to denote the state.  \n",
    "Each vertex will exist in one of three states:\n",
    "1. undiscovered – initial state. \n",
    "1. discovered – the vertex has been found, but not all incident edges visited.  \n",
    "1. processed – the vertex has been found, and all incident edges have been visited.  \n",
    "\n",
    "Traversal will be correct if all vertices are in the final (processed) state.  \n",
    "\n",
    "There are 2 general approaches: breadth-first and depth-first search\n",
    "1. *Breadth-first search (BFS)* - explore in order of distance to the root node.  \n",
    "1. *Depth-first search (DFS)* - explore all of an undiscovered subgraph before moving on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0942eb",
   "metadata": {},
   "source": [
    "# Breadth-First Search (BFS)\n",
    "\n",
    "In BFS, vertices are discovered in order of increasing distance from a source vertex.  \n",
    "This is typically implemented with a processing queue (i.e. FIFO order).  \n",
    "Below I write the abbreviated pseudocode for a BFS search on a graph `G`.  \n",
    "\n",
    "## BFS Pseudocode\n",
    "\n",
    ">**Input:** graph `G`, source vertex `s`.\n",
    ">1. Declare all other vertices to \"undiscovered.\"\n",
    ">1. Set state of `s` to \"discovered,\" no parent.\n",
    ">1. Put `s` into processing queue `Q`.\n",
    ">1. Until `Q` is empty, explore frontier:\n",
    ">    1. Dequeue node `u`.\n",
    ">    1. Process edges of `u`.\n",
    ">    1. Discover new child nodes `v`, parent `u`.\n",
    ">    1. Add any new nodes to `Q`.\n",
    ">1. After processing `u`, set to \"processed.\"\n",
    "\n",
    "BFS starts out exploring the source vertex (root).  \n",
    "From the source vertex, new adjacent vertices (children) are discovered and added to a processing queue.  \n",
    "Once the source is fully explored, the source vertex is marked \"processed.\"  \n",
    "We are left with vertices in the processing queue that are \"discovered\" but not \"processed.\"  \n",
    "I call these queued vertices the \"frontier,\" since they have (roughly) equal distance to the root and the search grows incrementally from it.  \n",
    "We explore the frontier vertices in the same way as the source, turning them into \"processed\" vertices and adding their children to the queue as well.  \n",
    "This processing continues until the queue is emptied (no more undiscovered children).  \n",
    "\n",
    "In this way, every vertex is discovered by at most one other vertex.  \n",
    "This will define a tree (actually a DAG) on vertices of the graph.  \n",
    "One can show that this will generate the shortest path from the root to every other node in the tree.  \n",
    "This is its main benefit over something like DFS.  \n",
    "\n",
    "## Correctness\n",
    "How do we know that BFS searches the entire graph?  \n",
    "For that matter, how do we know it gives the shortest paths from the source?  \n",
    "\n",
    "To establish this, turn to theorem 20.5 in Cormen, which I abbreviate below the terms.  \n",
    "Note that for a formal proof, you would have to go prove the lemmas the book does (I will just take them for granted here).  \n",
    "> **Terms:** \n",
    ">- $a.d$: The distance (read: number of edges) actually traversed from the source vertex $s$ to desired vertex $a$.  \n",
    ">Note that in BFS, the discoverer of a vertex has some distance $d_o$ while its newly discovered nodes have distance $d_o+1$.  \n",
    ">Due to the FIFO order of the queue, this also means the $d$ value of the currently processing vertex increases monotonically. \n",
    ">- $a.\\pi$: The predecessor (read: discoverer) of vertex $a$ in BFS. \n",
    ">- $\\delta(s, a)$: The smallest possible distance from source $s$ to desired vertex $a$.  \n",
    ">For a vertex to be *reachable*, this has to be a finite non-negative integer.   \n",
    "\n",
    "> **Theorem:**  \n",
    "Let $G = (V, E)$ be a graph, and suppose that BFS is run on $G$ from a given source vertex $s$.  \n",
    "Then, during its execution, BFS discovers every vertex $v \\in V$ that is reachable from the source $s$.  \n",
    "Moreover, the distance of $v$ to the source, $v.d$ is the shortest possible distance denoted $\\delta(s, v)$.  \n",
    "Furthermore, for any vertex $v \\neq s$ reachable from $s$, one of the shortest paths from $s$ to $v$ is a shortest path from $s$ to $v.\\pi$: followed by the edge $(v.\\pi, v)$.  \n",
    "> **Proof by Contradiction:**  \n",
    "Suppose there were an offending vertex $v$ s.t. the distance $v.d$ to the root were not minimal.  \n",
    "$$\\exists v|v.d > \\delta(s, v)$$\n",
    "Denoting its parent on the *actual* shortest path as $u$, it follows that \n",
    "$$\\delta(s, v) = \\delta(s,u) + 1 \\quad \\text{AND} \\quad u.d = \\delta(s, u)$$\n",
    "Putting them together\n",
    "$$v.d > \\delta(s, v) = \\delta(s,u) + 1 = u.d+1$$\n",
    "Then at the time $u$ is dequeued (read: fully processed), the offending vertex $v$ is potentially either undiscovered, discovered, or processed.  \n",
    "Each case leads to a contradiction.  \n",
    "**Case 1: $v$ undiscovered**  \n",
    "If $v$ is undiscovered, BFS has to add it to the queue before dequeing $u$ meaning $v.d = u.d + 1$.  \n",
    "But we declared $u$ is on the actual shortest path, in which case $u.d = \\delta(s,u) = \\delta(s, v) - 1 \\rightarrow v.d = \\delta(s, v)$.  \n",
    "This contradicts our initial assumption.  \n",
    "**Case 2: $v$ processed**  \n",
    "If $v$ is processed, it must have already gone through the queue.  \n",
    "But BFS searches in order of depth to the root, so this would mean $v.d \\leq u.d$ and $u$ is not really its parent on the shortest path.  \n",
    "This contradicts our definition of $u$ as the parent on the actual shortest path.  \n",
    "**Case 3: $v$ discovered**  \n",
    "If $v$ is discovered, but not processed, it must have been discovered by some other (processed) parent $w$ earlier than $u$.  \n",
    "This would require $w.d \\leq u.d$ and $v.d = w.d + 1$.  \n",
    "But then $v.d = w.d + 1 \\leq u.d + 1$.  \n",
    "This again contradicts our definition of $u$ as the parent on the actual shortest path.  \n",
    "\n",
    "> Thus we conclude that for BFS, $v.d = \\delta(s, v) \\forall v \\in V$.  \n",
    "All $v$ reachable from $s$ must (at some point) be discovered because the distance is upper-bounded by a finite number.   \n",
    "Finally, if $v.\\pi = u$, then $v.d = u.d+ 1$ and a shortest path from $s \\rightarrow v$ is the path $s \\rightarrow v.\\pi$, followed by a single edge $(v.\\pi, v)$.  \n",
    "**Q.E.D**\n",
    "\n",
    "## Applications of BFS\n",
    "\n",
    "Skiena goes over 2 applications of BFS, which I will briefly summarize and possibly make assessments of later.  \n",
    "1. Connectedness\n",
    "1. Vertex Coloring\n",
    "\n",
    "> **Checking Connectedness:** Given a graph, are all vertices reachable from any source?\n",
    "\n",
    "We say that a graph is *connected* if there is a path between any two vertices.   \n",
    "\n",
    "It turns out a lot of problems reduce to finding or counting connected components.  \n",
    "Skiena gives the example of legal Rubik’s cube configurations (solved and scrambled).  \n",
    "Actually a lot of [mechanical puzzles](https://en.wikipedia.org/wiki/God%27s_algorithm#Examples) can be thought of as graphs, and solving them is a form of search.  \n",
    "Really, anything endowed with a space of configurations could be considered a graph, with configuration/state being the vertex and legal moves being edges.  \n",
    "\n",
    "As another example, chess is solved with 7 or fewer pieces through what are known as [tablebases](https://en.wikipedia.org/wiki/Endgame_tablebase).  \n",
    "Such tablesbases are made through retrograde analysis: work backwards from the final position (checkmate or draw) and explore all legal positions.  \n",
    "With best play, every legal position is known to be either drawn or won within a minimum number of moves by connection in the configuration space.  \n",
    "In effect, this is BFS in reverse.  \n",
    "\n",
    "**Note:** While it is true you could do a BFS for things like Rubik's cube solving, I will clarify this is largely impractical beyond some solution depth.  \n",
    "[Korf (2008)](https://dl.acm.org/doi/10.1145/1455248.1455250) for instance taps out at about depth 10 solutions due to memory-constraints.  \n",
    "In case you were curious, [Rokicki et. al (2013)](https://tomas.rokicki.com/rubik20.pdf) showed that Rubik's cubes are always solvable within at most 20 moves (the so-called [God's number](https://www.cube20.org/)).  \n",
    "They solved it with a lot of mathematical reductions with groups and symmetries, then throwing 35 years of CPU processing at it.   \n",
    "\n",
    "> **Vertex Coloring Problem:** Given a graph, assign colors to each vertex s.t. no connected vertices have the same color. \n",
    "\n",
    "A coloring scheme which satisfies this property is called a *proper coloring* of the graph.  \n",
    "The smallest possible number of colorings, $\\chi(G)$, is the *chromatic number*.  \n",
    "A graph is *bipartite* if it can be properly colored using only two colors.  \n",
    "\n",
    "Such colorings show up in a lot of problems involving scheduling or network coverage.  \n",
    "[This publication](https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1979.tb32824.x) was one of the earliest ones I could find that explicitly treats them as such (should also be available in the course page).  \n",
    "More easily digestible explanations of the telecom application are shown [here](https://www.cs.kent.edu/~dragan/ST-Spring2016/Allocating%20radio%20frequencies%20using%20graph%20coloring.pdf) and [here](https://www.cs.kent.edu/~dragan/ST-Spring2016/Frequency%20Assignment%20in%20cellular%20networks%20(1).pdf).\n",
    "\n",
    "BFS (if you tweak it a bit) also happens to be quite good at checking if a graph is bipartite because of the frontier.  \n",
    "Simply alternate the color of frontier vertices with their child vertices, and check for any conflicts in non-discovering edges.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bbe4b",
   "metadata": {},
   "source": [
    "# Depth-First Search (DFS)\n",
    "\n",
    "DFS can be identified with backtracking in a maze. \n",
    "1. Pick an unexplored direction/node. \n",
    "1. Advance as far as possible in the same fashion. \n",
    "1. If stuck, back up to the last unexplored direction.  \n",
    "\n",
    "Both are most easily understood/implemented as recursive algorithms, i.e. with a stack.  \n",
    "Below I write the abbreviated pseudocode for a DFS search on a graph `G`.\n",
    "\n",
    "## DFS Pseudocode\n",
    "\n",
    ">**Input:** graph `G`, vertex `u`. \n",
    ">1. Set state of `u` to \"discovered.\" \n",
    ">1. Process edges of `u`. \n",
    ">1. For all children `v` of `u`:\n",
    ">    1. Set parent of `v` to `u`.\n",
    ">    1. If undiscovered, recursive call on `v`. \n",
    ">1. Set state of `u` to \"processed.\"  \n",
    "\n",
    "Unlike BFS, where we had an explicit queue object `Q`, here the stack management is done with whatever call stack the system you're using has.  \n",
    "In Skiena, he also times the execution of the function as a way to measure the number of descendents.  \n",
    "I omitted because you can do that yourself, but it is somewhat important to proving some theorems so bear them in mind.  \n",
    "\n",
    "## Edge Classification\n",
    "Skiena also mentions (oddly late into the chapter) that while DFS is simple, the actual details of implementation are subtle.  \n",
    "This is mostly to do with the timings of processing edges and vertices.  \n",
    "Consider an edge from the current vertex `x` to another vertex `y`.  \n",
    "It is straightforward if `y` is undiscovered vertex; this must be the first time we're seeing it.  \n",
    "But what happens if `y` is an already discovered: have we already run through it?  \n",
    "Worse yet, is this an ancestor of the current vertex, and we're making more redundant work for ourselves? \n",
    "\n",
    "Apparently one can work out the different cases by \"careful reflection.\"  \n",
    "Perhaps it is more cautionary than instructional.  \n",
    "So, yes, think very carefully about how to actually implement a DFS-based algorithm in code.  \n",
    "\n",
    "But from the theory perspective, you can at least know that DFS is correct with theorems 20.9 and 20.10 in Cormen.  \n",
    "These theorems establish that in a DFS search, it creates a search tree and there is enough information to classify the edges into 4 categories.  \n",
    "1. **Tree edges:** $(u, v)$ is a tree edge if $v$ was first discovered by exploring this edge. \n",
    "2. **Back edges:** $(u, v)$ is a back edge if it connects descendent vertex $u$ to ancestor $v$.\n",
    "3. **Forward edges:** nontree edges connecting a vertex $u$ to a proper descendant $v$ in a tree.\n",
    "4. **Cross edges:** all other edges.  \n",
    "They can go between vertices in the same tree, as long as one vertex is not an ancestor of the other, or they can go between vertices in different trees.\n",
    "\n",
    "The end result is that properly implemented DFS should only ever generate the first two types of edge, in which case, the resolutions should be manageable.  \n",
    "\n",
    "Now, I am *not* going to go over the proofs as I did for BFS.  \n",
    "This time I will ask you to go over Cormen and do the same type of reading/understanding of the proof.  \n",
    "Write up a similar summary for exercise.  \n",
    "\n",
    "## Applications of DFS\n",
    "\n",
    "Skiena goes over 2 applications of DFS, which I will briefly summarize and possibly make assessments of later.  \n",
    "1. Cycle Detection\n",
    "1. Articulation Vertices\n",
    "\n",
    "> **Cycle detection:** Given a graph, does it contain any cycles?  \n",
    "\n",
    "Cycles are somewhat important to detect, usually for warning systems.  \n",
    "In most graph applications specifically, you want to avoid cycles because they are associated with non-terminating states.  \n",
    "For something like version control (i.e. github), which stores the branches and forks as a DAG, a cycle would be pretty bad for tracking history.  \n",
    "\n",
    "The benefit of DFS is that it organizes the edges of the graph in a precise way.  \n",
    "Vertices are preferentially discovered by depth, and edges are strictly tree or back edges.  \n",
    "If there is at all a back edge, there is clearly a cycle.  \n",
    "\n",
    "> **Articulation Vertices:** Given a graph, does it contain articulation vertices? \n",
    "\n",
    "Suppose you had a network (generators, computers, supply chains, etc.).  \n",
    "Suppose a catastrophe knocks out one of the nodes. How affected is the service?  \n",
    "Strongly connected graphs will be resilient.  \n",
    "Linear graphs may be completely bisected.  \n",
    "\n",
    "An *articulation vertex* or *cut node* is a vertex whose removal will disconnect the graph.  \n",
    "Identifying the existence of such vertices is clearly important for people who care about the stability of their network.  \n",
    "We can find them by brute force: for all nodes, delete one and BFS/DFS the rest.  \n",
    "But we can do better (linear time) with a single DFS. \n",
    "\n",
    "$v$ is an articulation vertex/cut node if, and only if either\n",
    "- $v$ is a root with more than one child, OR\n",
    "- $v$ is not a root or leaf, and subtrees of $v$ do not have a back edge to ancestors of $v$. \n",
    "\n",
    "Graphs without articulation vertices are *biconnected.* \n",
    "\n",
    "## Applications to Directed Graphs\n",
    "\n",
    "Skiena goes over 2 applications of DFS, which I will very shortly summarize, and mostly leave you to read because they seem straightforward.  \n",
    "1. Topological Sorting on DAGs\n",
    "1. Strongly Connected Components\n",
    "\n",
    "For the first, a directed acyclic graph (DAG) has no directed cycles, and a topological sorting is an orderings on vertices s.t. all edges go left-to-right.  \n",
    "DFS produces only tree edges on DAGs, so by assigning a number in the order a vertex is marked \"processed,\" you naturally produce a topological sorting.  \n",
    "Note though that DFS is not intrinsically a deterministic sort because of the branching of the tree, so there may be many equivalent sorts.  \n",
    "\n",
    "For the second, strongly connected components are sections of the graph for which there is a path going to and from any pair of vertices.  \n",
    "This is applicable to things like road planning: one hopes that a road network allows you to get around a city without having to go off-road/violate traffic laws.  \n",
    "To test for strongly connected components, simply use DFS on the directed graph and the graph with edges reversed.  \n",
    "For any given vertex $v$, these will give the paths from and to it respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e7b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
