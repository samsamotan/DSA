{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf07ebf8",
   "metadata": {},
   "source": [
    "### Author: Jose Miguel Bautista\n",
    "### Updated: 05/31/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf40d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a748f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skiena graph from Figure 6.3 \n",
    "# Used in all subsequent cells for consistency, but you may swap out with arbitrary square matrices for experimentation. \n",
    "test = np.array([\n",
    "    [0, 5, 7, 9, 0, 0, 0], \n",
    "    [0, 0, 0, 7, 12, 0, 0],\n",
    "    [0, 0, 0, 4, 0, 2, 5],\n",
    "    [0, 0, 0, 0, 4, 3, 0],\n",
    "    [0, 0, 0, 0, 0, 7, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 2],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "], dtype=float)\n",
    "test = (test+test.T)\n",
    "\n",
    "# Depending on the code used, either 0 or infinity is more conveninent for marking non-connection. \n",
    "# By default I will use infinity, but swap as needed. \n",
    "test[test == 0] = np.inf \n",
    "np.fill_diagonal(test, 0) # necessary for shortest paths, not strictly for MST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c095fbb",
   "metadata": {},
   "source": [
    "# Weighted Graphs\n",
    "\n",
    "Our work up to this point has been on simple, unweighted graphs.  \n",
    "Such graphs can be represented mathematically with boolean adjacency matrices.  \n",
    "Weighted graphs can be represented by the same system, except instead of boolean entries, we log the weights in a *distance matrix*.  \n",
    "Such weights can convey more information about the relationship (e.g. correlation strength, physical distance, etc.).  \n",
    "This construction also requires us to rethink some of the algorithms and potential problems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ae861",
   "metadata": {},
   "source": [
    "# Minimum Spanning Trees\n",
    "\n",
    "Recall that a tree is a connected graph without cycles.  \n",
    "A *spanning tree* is a subgraph of G with all the same vertices of $G$ (but not the same edges), and is itself a tree.  \n",
    "A *minimum spanning tree (MST)* is the spanning tree of weighted graph $G$ s.t. sum of edges is minimized.  \n",
    "**Note:** The MST is not necessarily unique, such as if the graphs has multiple edges of the same weight.  \n",
    "\n",
    "This has practical applications to network design.  \n",
    "Historically, MSTs were applied to electrical grids in Moravia (Czech republic), by [Boruvka](https://en.wikipedia.org/wiki/Bor%C5%AFvka%27s_algorithm).  \n",
    "You clearly want to have a grid that is connected, in order to maintain a proper service.  \n",
    "But wiring all of them together takes time, effort, and literal miles of wiring.  \n",
    "Given a set of power stations, how do you connect them in a way that requires the least amount of wire?    \n",
    "\n",
    "Boruvka managed to do this in the 60s, without the aid of computers.  \n",
    "What this means for us is that we're *not* going to go over it because it's more complicated (for now).  \n",
    "Once we get through the relatively simpler algorithms, you can go look at his work in your own time.  \n",
    "\n",
    "\n",
    "## Prim's Algorithm\n",
    "\n",
    "Prim's algorithm is one of the simplest methods to make an MST.  \n",
    "\n",
    "**Pseudocode:**\n",
    "1. Pick a starting vertex (arbitrary).\n",
    "1. While there are unvisited vertices:\n",
    "    1. Greedily select the lowest weight edge between a tree and nontree vertex. \n",
    "    1. Add the edge and corresponding vertex to the current tree.\n",
    "\n",
    "### Proof of Correctness\n",
    "Clearly Prim's algorithm creates a spanning tree because \n",
    "1. It only adds edges from tree to nontree vertices, and\n",
    "1. It adds until all vertices are included.\n",
    "Is it minimal?\t\n",
    "\t\n",
    ">**Theorem: Correctness of Prim's Algorithm**  \n",
    "Given a graph $G$ with only positive edges, Prim's algorithm correctly identifies its minimum spanning tree.  \n",
    "**Proof by Induction:**  \n",
    "First consider the case of the subgraph consisting of 2 vertices sharing an edge.  \n",
    "Prim's algorithm has no choice but to select the lone edge, which will be an MST, so the base case is shown.  \n",
    "Now for the inductive step, assume Prim's algorthm has already constructed an MST on a subgraph.  \n",
    "In the next step, Prim's algorithm will select the minimum edge to an unvisited vertex.  \n",
    "Because there are only positive edges, the net path distance is strictly increasing in path length.  \n",
    "Consequently, the shortest paths between disjoint sets of vertices have to be of path length 1.  \n",
    "The resulting tree must therefore be an MST on the (larger) subgraph.  \n",
    "Thus, Prim's algorithm produces a correct MST.  \n",
    "**QED**\n",
    "\n",
    "I will clarify that Skiena's proof by contradiction uses the same argument, where selected edges cannot be incorrect because the edges are positive.  \n",
    "This leads to a more general idea of why a greedy algorithm may/may not fail.  \n",
    "Greedy algorithms generally fail because they get \"stuck\" in locally optimal solutions.  \n",
    "For MSTs on graphs with positive weights, the algorithm can't get stuck because no negative edges (that lower net path distance) exist. \n",
    "\n",
    "### Performance\n",
    "How fast Prim's algorithm runs depends on the implementation/data structures.  \n",
    "Below I have some demo code that takes a graph as an adjacency matrix (dense), and does a brute force check for the next edge to add.  \n",
    "This should remind you of the selection sort algorithm, and the way it is sped up by using a better data structure.  \n",
    "In fact, the fastest way to do this is also using heaps, which I demo below as well.  \n",
    "**Exercise:** Figure out the time complexities, both from analysis of the code, and numerically.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c87d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 µs ± 5.91 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(0, 1, 5.0),\n",
       "  (0, 2, 7.0),\n",
       "  (2, 5, 2.0),\n",
       "  (5, 6, 2.0),\n",
       "  (5, 3, 3.0),\n",
       "  (3, 4, 4.0)],\n",
       " 23.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: has no guardrails for disconnected/negative graphs. \n",
    "\n",
    "def prim(adj_matrix, start_vertex):\n",
    "    #print(\"Starting vertex: \", start_vertex)\n",
    "    v_num = len(adj_matrix)\n",
    "    in_tree = [start_vertex] \n",
    "    edge_list = []\n",
    "    net_weight = 0\n",
    "    \n",
    "    while len(in_tree)< v_num:\n",
    "        best_weight = np.inf\n",
    "        best_edge = None\n",
    "        for i in in_tree:\n",
    "            for j in range(v_num):\n",
    "                weight = adj_matrix[i, j]\n",
    "                if (j not in in_tree)&(weight < best_weight):\n",
    "                    best_weight = weight \n",
    "                    best_edge = (i, j, weight)\n",
    "                    \n",
    "        v1, v2, weight = best_edge\n",
    "        net_weight += best_weight\n",
    "        in_tree.append(v2)\n",
    "        edge_list.append(best_edge)\n",
    "        \n",
    "    return edge_list, net_weight\n",
    "    \n",
    "test[test == 0] = np.inf\n",
    "np.fill_diagonal(test, 0)\n",
    "\n",
    "%timeit outEdge, outWeight = prim(test, 0)\n",
    "prim(test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7756b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4 µs ± 162 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(0, 1, 5.0),\n",
       "  (0, 2, 7.0),\n",
       "  (2, 5, 2.0),\n",
       "  (5, 6, 2.0),\n",
       "  (5, 3, 3.0),\n",
       "  (3, 4, 4.0)],\n",
       " 23.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improved version that takes csr format and uses heaps to organize edge exploration\n",
    "# Rather than rechecking for connecting edges every time, it just logs them into a heap\n",
    "# Note: still has no guardrails\n",
    "\n",
    "def prim_csr(data, indices, indptr, start_vertex):\n",
    "    #print(\"Starting vertex: \", start_vertex)\n",
    "    \n",
    "    v_num = len(indptr)-1\n",
    "    in_tree = [start_vertex]\n",
    "    visited = [False] * v_num\n",
    "    visited[start_vertex] = True\n",
    "    edge_heap = []\n",
    "    edge_list = []\n",
    "    net_weight = 0\n",
    "\n",
    "    def add_edges(u):\n",
    "        for i in range(indptr[u], indptr[u + 1]):\n",
    "            v = indices[i]\n",
    "            w = data[i]\n",
    "            if not visited[v]:\n",
    "                heapq.heappush(edge_heap, (w, u, v))\n",
    "\n",
    "    add_edges(start_vertex)\n",
    "\n",
    "    while (len(in_tree) < v_num):\n",
    "        w, u, v = heapq.heappop(edge_heap)\n",
    "        if visited[v]:\n",
    "            continue\n",
    "        visited[v] = True\n",
    "        in_tree.append(v)\n",
    "        edge_list.append((u, v, w))\n",
    "        net_weight += w\n",
    "        add_edges(v)\n",
    "\n",
    "    return edge_list, net_weight\n",
    "\n",
    "test[np.isinf(test)] = 0 # note: csr_matrix only recognizes 0s for non-connection\n",
    "test_csr = csr_matrix(test)\n",
    "data = test_csr.data\n",
    "indices = test_csr.indices  \n",
    "indptr = test_csr.indptr\n",
    "  \n",
    "%timeit prim_csr(data, indices, indptr, 0)\n",
    "prim_csr(data, indices, indptr, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f43e7e",
   "metadata": {},
   "source": [
    "## Kruskal's Algorithm\n",
    "\n",
    "Kruskal's algorithm is an alternative to Prim's algorithm.  \n",
    "It is actually similar to the heap-variant of Prim, except now we heap from the get-go.  \n",
    "Then we add edges, forming (at least initially) disconnected trees.  \n",
    "We keep adding edges as long as the edge does not connect a tree to itself. \n",
    "\n",
    "**Pseudocode:**\n",
    "1. Put all the edges in a priority queue by weight.\n",
    "1. While the queue is non-empty:\n",
    "    1. Test if the edge connects 2 vertices in the same tree\n",
    "    1. Add the edge if it does not, reject if it does.\n",
    "\n",
    "### Exercise - Proof of Correctness\n",
    "Clearly Kruskal's algorithm creates a spanning tree because \n",
    "1. It only adds tree edges, explicitly avoiding back/cross edges\n",
    "1. All reachable vertices have to be added (a lone vertex is always a disconnected tree)\n",
    "\n",
    "I will now ask you to prove that such a spanning tree is minimal.  \n",
    "See Skiena p. 197 if you really need the answer, but I insist you work it out in the same way the proof of Prim's algorithm.  \n",
    "    \n",
    "### Performance\n",
    "How fast Kruskal's algorithm runs depends (again) on implementation and choice of data structures.  \n",
    "You could code up your own version, and you should at some point to get the order of addition right.  \n",
    "But I will at this point note that scipy's `minimum_spanning_tree` does use Kruskal's algorithm.  \n",
    "Below I have demo code for it.  \n",
    "\n",
    "What you should notice is that it is faster than our naive Prim, but slower than our heaped Prim.  \n",
    "**Exercise:** Why is it slower? Be precise in your statements. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5e7b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 µs ± 1.09 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 5, 7, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 2, 0],\n",
       "       [0, 0, 0, 0, 4, 3, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 2],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit minimum_spanning_tree(test_csr)\n",
    "minimum_spanning_tree(test_csr).toarray().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8fe96",
   "metadata": {},
   "source": [
    "## Union-find\n",
    "The slow part of Kruskal’s algorithm in the book (initially) is the connectivity check, which is done by a breadth-first search.   \n",
    "This is overkill, because we only need to test if 2 vertices to be connected are in same tree + merge, not a full search.  \n",
    "These can be implemented by union and find operations on sets, specifically data structures called [disjoint sets](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.DisjointSet.html), where \n",
    "- `FIND(i)` – Return the root of tree containing element i\n",
    "- `UNION(i,j)` – Link the root of one of the tree (i) to the root of the other (j).  \n",
    "\n",
    "Internally, such structures are disconnected trees where the pointers run from child to parent.  \n",
    "By following the pointer from any descendent node, `FIND` returns the \"representative\" of its set (the root node).  \n",
    "Thus we only merge if the representatives of 2 sets are different.  \n",
    "If we want union and find to be fast, it is best to maintain trees of low height.  \n",
    "When merging trees $T_1$ and $T_2$ into new tree $T_3$, which one becomes the root? \n",
    "- Shorter tree: `HEIGHT`($T_3$) = `MAX`( `HEIGHT`($T_1$), `HEIGHT`($T_2$) ) $+ 1$\n",
    "- Taller tree: `HEIGHT`($T_3$) = `MAX`( `HEIGHT`($T_1$), `HEIGHT`($T_2$) )  \n",
    "\n",
    "Ergo, we should keep the taller tree as the root.  \n",
    "Scipy uses a union-find structure in its implementation, but it also has 2 other techniques to speed things up.  \n",
    "These are mentioned in the source code, and explained in Cormen 19, so I will go over them. \n",
    "\n",
    "### Union-by-Rank\n",
    "Remember that we want to maintain trees of low height to keep the operations on the union-find as fast as possible.  \n",
    "So typically some space is allocated to log the number of descendants per tree node for fast comparison.  \n",
    "But note that the height of the tree grows (by 1) if and only if the input trees are the same height.  \n",
    "So it is more actually efficient to store *ranks* rather than full sizes.  \n",
    "\n",
    "A rank is initialized at 0 for all trees (lone vertices).  \n",
    "When two trees of the same height are connected by an edge, either may become the parent of the other.  \n",
    "In that case, the parent tree has its rank increased by 1, and the child's rank never changes beyond that point.  \n",
    "As such, rank is an upper bound of tree height.  \n",
    "\n",
    "### Path Compression\n",
    "\n",
    "Remember that our only goal is to find the representative for checking connectivity.  \n",
    "We want to use union-find over BFS because we don't need to search every path.  \n",
    "\n",
    "But for that matter, we don't need to copy the actual graph structure into our union-find.  \n",
    "In other words, we don't care about parents and grandparents in the actual graph, we just need to know the representative of the tree.  \n",
    "We may as well make the representative the parent of all descendants in our union-find structure, which always has height 1. \n",
    "\n",
    "Again, the height only increases if merging trees of the same height, after which one may compress the tree.  \n",
    "It should be clear that this compression can be done recursively (if the node is not the root, recursively check its parent).  \n",
    "This pairs nicely with the union-by-rank method above, because rank is also decoupled from direct tree structure.  \n",
    "Combining the two yields a theoretical time complexity that is nearly linear.  \n",
    "It actually goes as an [inverse Ackermann function](https://en.wikipedia.org/wiki/Ackermann_function#Inverse), $\\alpha(n)$, which is effectively a constant for reasonable input.  \n",
    "See Cormen 19.4 or Tarjan’s work for the full details.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2b478",
   "metadata": {},
   "source": [
    "# Shortest Paths\n",
    "\n",
    "A path is a sequence of edges connecting two vertices.  \n",
    "Finding the shortest path between vertices is a common problem:\n",
    "- Google Maps - what is the fastest route between 2 locations.\n",
    "- IP Routing - same as mapping\n",
    "- Video games - how a character move to get somewhere when there are obstacles\n",
    "- Linguistics - [how does one learn complex words?](https://pubmed.ncbi.nlm.nih.gov/39465458/)\n",
    "- Bioinformatics - [how are different gene sets related?](https://pmc.ncbi.nlm.nih.gov/articles/PMC5726383/)\n",
    "\n",
    "On unweighted graphs, we can do this in $O(n+m)$ using BFS.  \n",
    "Weighted graphs have some complications.  \n",
    "The weight of a path between two vertices is the sum of the weights of the edges on a path.  \n",
    "But now the contributions of edges are no longer uniform.  \n",
    "BFS will not work here because now, a sum of mutliple edges can be less than a single direct (but heavy) edge. \n",
    "\n",
    "Cycles with net negative weights render a \"shortest path\" meaningless.  \n",
    "If a negative cycle exists, we can always travel around it more to arbitrarily reduce the weight of final path.  \n",
    "This has applications to triangular arbitrage, but makes our immediate goal more complicated.  \n",
    "Thus (initially) we will assume that all edge weights are positive.  \n",
    "**Note:** Our minimum spanning tree algorithms are unaffected by negative edges.\n",
    "\n",
    "In [scipy](https://github.com/scipy/scipy/blob/main/scipy/sparse/csgraph/_shortest_path.pyx), there are different algorithms to compute the shortest path, and we will briefly go over most of them.  \n",
    "Below I have some demo code for the supported algorithms, though scipy can automatically choose which one to use based on what it thinks will be fastest.  \n",
    "I will note that at the time of writing, its implementation of Dijkstra and Johnson's algorithms do not work if the associated matrices are not symmetric.  \n",
    "So you will need to make your own versions for those if you need an asymmetric (read: directed) graph.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ac7b04-1d60-4fec-a0e4-97bd59cd2cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dijkstra's Algorithm \n",
      "\n",
      "Distance from vertex/es (0):  [ 0.  5.  7.  9. 13.  9. 11.]\n",
      "Predecessor on shortest path:  [-9999     0     0     0     3     2     5]\n",
      "616 µs ± 64.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "test[np.isinf(test)] = 0\n",
    "\n",
    "print(\"Dijkstra's Algorithm \\n\")\n",
    "dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, indices=start, return_predecessors=True, method = 'D')\n",
    "print(\"Distance from vertex/es ({}): \".format(start), dist_matrix)\n",
    "print(\"Predecessor on shortest path: \", predecessors) #-9999 indicates no path\n",
    "%timeit dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, indices=start, return_predecessors=True, method = 'D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c040311c-2a9b-4e71-a811-4bdcba3109a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floyd-Warshall Algorithm \n",
      "\n",
      "Distance from vertexes: \n",
      " [[ 0.  5.  7.  9. 13.  9. 11.]\n",
      " [ 5.  0. 11.  7. 11. 10. 12.]\n",
      " [ 7. 11.  0.  4.  8.  2.  4.]\n",
      " [ 9.  7.  4.  0.  4.  3.  5.]\n",
      " [13. 11.  8.  4.  0.  7.  9.]\n",
      " [ 9. 10.  2.  3.  7.  0.  2.]\n",
      " [11. 12.  4.  5.  9.  2.  0.]]\n",
      "Predecessor on shortest path: \n",
      " [[-9999     0     0     0     3     2     5]\n",
      " [    1 -9999     3     1     3     3     5]\n",
      " [    2     3 -9999     2     3     2     5]\n",
      " [    3     3     3 -9999     3     3     5]\n",
      " [    3     3     3     4 -9999     4     5]\n",
      " [    2     3     5     5     5 -9999     5]\n",
      " [    2     3     5     5     5     6 -9999]]\n",
      "291 µs ± 8.09 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#start = 0 # must input full graph\n",
    "test[np.isinf(test)] = 0\n",
    "\n",
    "print(\"Floyd-Warshall Algorithm \\n\")\n",
    "dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, return_predecessors=True, method = 'FW')\n",
    "print(\"Distance from vertexes: \\n\", dist_matrix)\n",
    "print(\"Predecessor on shortest path: \\n\", predecessors) #-9999 indicates no path\n",
    "%timeit dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, return_predecessors=True, method = 'FW')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37175038-58f6-4d3a-9273-09a36ab4e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellman-Ford Algorithm \n",
      "\n",
      "Distance from vertex/es (0):  [ 0.  5.  7.  9. 13.  9. 11.]\n",
      "Predecessor on shortest path:  [-9999     0     0     0     3     2     5]\n",
      "419 µs ± 26.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "test[np.isinf(test)] = 0\n",
    "\n",
    "print(\"Bellman-Ford Algorithm \\n\")\n",
    "dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, indices=start, return_predecessors=True, method = 'BF')\n",
    "print(\"Distance from vertex/es ({}): \".format(start), dist_matrix)\n",
    "print(\"Predecessor on shortest path: \", predecessors) #-9999 indicates no path\n",
    "%timeit dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, indices=start, return_predecessors=True, method = 'BF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce562c9b-c9c7-4f94-afa2-09a87dc9a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johnson's Algorithm \n",
      "\n",
      "Distance from vertex/es (0):  [ 0.  5.  7.  9. 13.  9. 11.]\n",
      "Predecessor on shortest path:  [-9999     0     0     0     3     2     5]\n",
      "603 µs ± 13 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "test[np.isinf(test)] = 0\n",
    "\n",
    "print(\"Johnson's Algorithm \\n\") # skipped in discussion\n",
    "dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, indices=start, return_predecessors=True, method = 'J')\n",
    "print(\"Distance from vertex/es ({}): \".format(start), dist_matrix)\n",
    "print(\"Predecessor on shortest path: \", predecessors) #-9999 indicates no path\n",
    "%timeit dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, indices=start, return_predecessors=True, method = 'J')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1297f76-5150-4087-9f11-aa2826d74a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto Select\n",
      "\n",
      "Distance from vertex/es (0):  [ 0.  5.  7.  9. 13.  9. 11.]\n",
      "Predecessor on shortest path:  [-9999     0     0     0     3     2     5]\n",
      "590 µs ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "test[np.isinf(test)] = 0\n",
    "\n",
    "print(\"Auto Select\\n\")\n",
    "dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, indices=start, return_predecessors=True, method = 'auto')\n",
    "print(\"Distance from vertex/es ({}): \".format(start), dist_matrix)\n",
    "print(\"Predecessor on shortest path: \", predecessors) #-9999 indicates no path\n",
    "%timeit dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, indices=start, return_predecessors=True, method = 'auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322b67b",
   "metadata": {},
   "source": [
    "## Dijkstra's Algorithm\n",
    "\n",
    "The basic method for shortest paths is Dijkstra's algorithm, and it stems from the following observation:  \n",
    ">Suppose a path $(s, \\ldots, x, \\ldots, t)$ is the shortest path from $s$ to $t$.  \n",
    "Then the subpath $(s, \\ldots, x)$ must be the shortest path from $s$ to $x$ on the way to $t$.   \n",
    "If it were not, we could take the actual shortest path to $x$, then use the other half of the original path $(x, \\ldots, t)$.\n",
    "\n",
    "This suggests a *dynamic programming* approach.  \n",
    "Dijkstra's algorithm proceeds in a series of rounds, each finding the shortest path from source $s$ to new vertex $x$.   \n",
    "More distant vertices will have paths constructed from closer ones.   \n",
    "\n",
    "**Pseudocode:**  \n",
    "1. Initialize distance to source $s$.  \n",
    "This is $0$ for $s$ itself, infinity for every other vertex.  \n",
    "1. Set the unvisited vertex with smallest edge as `LAST` (initialized as $s$).  \n",
    "1. From `LAST`, calculate distances to connected vertices going through `LAST`.  \n",
    "1. For connected vertices, compare new distances to the current, and assign the smaller value.  \n",
    "1. Mark `LAST` as visited.  \n",
    "1. Repeat from step 2 until target vertex found.  \n",
    "\n",
    "You might notice a similarity to Prim’s algorithm (both are greedy).  \n",
    "Per iteration, you add exactly one vertex to the tree of best solutions in order of increasing cost.  \n",
    "The difference is how they prefer/rate new vertices.  \n",
    "- For Prim/MST, we only care about the (marginal) weight being added by potential edges. \n",
    "- For Kruskal/shortest path, we care about the overall path distance to source.\n",
    "\n",
    "As a note on implementation, you can make your own version of Dijkstra's algorithm with priority, possibly following [this version](https://www.geeksforgeeks.org/dijkstras-shortest-path-algorithm-greedy-algo-7/).  \n",
    "But in the scipy implementation, they use a special data structure called [Fibonacci heaps](https://en.wikipedia.org/wiki/Fibonacci_heap) to improve their asymptotic run times.   \n",
    "We are not going to go over them, because it would take a while.  \n",
    "But the basic idea is to use mutliple min-heaps (instead of just one) and handle them lazily, merging as needed.  \n",
    "Effectively, the height of any heap in a Fibonacci heap is (on average) smaller than a full priority queue which should speed it up (but see the appendix). \n",
    "  \n",
    "## Floyd-Warshall Algorithm and Distance Matrices\n",
    "\n",
    "Dijkstra’s algorithm finds the shortest path between a pair of vertices, or more geenrally from any given source.  \n",
    "Sometimes we need the shortest path between all pairs of vertices, such as if we want to characterize the \"size\" of the graph.  \n",
    "We could run Dijkstra’s algorithm for all pairs, but we may be able to do better with math. \n",
    "\n",
    "### Distance Matrices And Distance Products\n",
    "Similar to the adjacency matrix, for (weighted) graph $G(V, E)$, we can construct the distance matrix $D$:\n",
    "\\begin{align}\n",
    "D[i, j] = 0 \\,\\,\\, \\quad \\quad &\\text{IF} \\,\\, i = j\\\\\n",
    "D[i, j] = \\infty \\quad \\quad &\\text{IF} \\,\\, (i \\neq j)\\text{ AND }(v_i, v_j) \\in E\\\\\n",
    "D[i, j] = w(i, j)\\,\\, &\\text{IF} \\,\\, (v_i, v_j) \\in E\n",
    "\\end{align}\n",
    "where $w(i, j)$ is the weight of the edge going from vertex $i$ to vertex $j$.  \n",
    "This alone tells us the shortest direct path (no intermediate vertices).  \n",
    "Unlike the adjacency matrix, powers of this matrix under [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) are not that useful.  \n",
    "\n",
    "Instead, the relevant operation you want to reach for is [min-plus multiplication](https://en.wikipedia.org/wiki/Min-plus_matrix_multiplication).  \n",
    "This has some fancy names associated with it like \"tropical semi-rings\" but you don't need to worry about them.  \n",
    "All you need to know is how to calculate it, which is functionally the same as the usual matrix product (read: a collection of dot products).  \n",
    "The only differences are:\n",
    "1. Replace all addition operations with `MIN` comparisons, and\n",
    "2. Replace all multiplications with addition.\n",
    "\n",
    "### Floyd-Warshall (FW)\n",
    "Say we want to get from vertex $i$ to $j$, and assume we need to pass (at least) vertex $k$ on the best path.  \n",
    "Key idea: treat all possible vertices $k$ as an intermediate node sequentially.  \n",
    "When considering vertex $k$, vertices $0$ to $k-1$ will have already been processed.  \n",
    "As before, shortest paths from vertices $0$ to $k-1$ will build the shortest paths to vertex $k$.   \n",
    "\n",
    "**Pseudocode:**  \n",
    "1. Initialize $V\\times V$ distance matrix $D$. \n",
    "    1. Set all diagonal elements to $0$. \n",
    "    1. Assign appropriate weights to relevant matrix positions. \n",
    "    1. Set all other entries to infinity. \n",
    "1. For all possible intermediate vertices $k$:\n",
    "    1. For all pairs of starting and ending vertices $(i, j)$ excluding k:\n",
    "        1. Add distance $i\\rightarrow k$ to distance $k \\rightarrow j$. \n",
    "        1. If this value is lower than current distance $i\\rightarrow j$, replace it. \n",
    "\n",
    "Nominally, FW is not faster than just running Dijkstra’s algorithm for every pair.  \n",
    "If anything, it should be slower because Dijkstra's coukld be sped up with special data structures but this is necessarily O(V^3)$.  \n",
    "As you see in the demo above, it can (counterintuitively) be faster.  \n",
    "A further benefit is that it turns out FW can handle negative edges, as long as no negative cycles are present  \n",
    "Dijkstra is not designed to handle any negative edges at all.  \n",
    "In fact, FW can tell you if negative cycles are present because they will show up as negative diagonals.  \n",
    "\n",
    "## Bellman-Ford (BF) Algorithm \n",
    "\n",
    "Dijkstra’s algorithm fails for negative edges because it never revisits nodes.  \n",
    "With the distance matrix formalism, FW algorithm can overcome this.  \n",
    "Bellman-Ford is a mix, in that it uses distance prioduct, but it only finds distances to one source rather than a full matrix.  \n",
    "Like Dijkstra's algorithm, BF relaxes solutions, but relaxes all edges (not just outgoing ones).\n",
    "\n",
    "**Pseudocode:**\n",
    "1. Initialize a distance array ($0$ for itself, else infinity), denoted $d$.\n",
    "1. For all edges $(u, v)$:\n",
    "    1. Consider distance $d[u]$ of vertex $u$ to source $s$, and add any potential edges $w(u,v)$.\n",
    "    1. Compare to all current distances $d[v]$ of vertex $v$ to source $s$.  \n",
    "       If any potential edges make the source distance smaller, update $d[v]$.  \n",
    "1. Repeat step 2, $V-1$ times.\n",
    "\n",
    "You should note that this will make the corresponding row of the distance matrix from Floyd-Warshall.  \n",
    "This is for a good reason: both FW and BF calculate a min-plus product.  \n",
    "Whereas FW does it between a (distance) matrix and itself, BF does it between the matrix and a vector (a row of the distance matrix itself). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069f223-4858-42ce-a19c-de1400e44023",
   "metadata": {},
   "source": [
    "# Appendix: Distance Product\n",
    "\n",
    "By default, Python has no min-plus products, and unfortunately neither do scipy or numpy.  \n",
    "So if we want to use it to calculate shortest paths (of arbitrary max path length), we need to make it ourselves.  \n",
    "In the interest of encouraging you to look at stackexchange and stackoverflow more, and for instructional purposes, here are [some](https://stackoverflow.com/questions/47359743/how-to-make-min-plus-matrix-multiplication-in-python-faster) of the solutions I got there.  \n",
    "\n",
    "As a brief description of each approach:\n",
    "1. The first version is a basic implementation using column-wise loops. \n",
    "1. The second version improves the first by removing explicit loops via reshaping and broadcasting. \n",
    "1. The third version is similar to the second, though the reshaping uses `newaxis` instead of `expanded_dims`, as it is evidently much faster.  \n",
    "I assume this is due to lack of function overhead and possibly cache locality, though I am not entirely certain.  \n",
    "\n",
    "To verify that they work, I calculate to the $(V-1)$-th power and compare to the output from scipy's Floyd-Warshall.  \n",
    "The actual times should vary depending on your systems, and if you use different inputs.  \n",
    "On my setup, compared to the first version, there is about a factor of $\\approx$ 2 and $\\approx$ 5 for the second and third versions respectively.   \n",
    "**Moral of the Story:** broadcasting is awesome, avoid loops where possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295eccc5-ac2d-4ec7-83f2-337c459a1fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  5.  7.  9. 13.  9. 11.]\n",
      " [ 5.  0. 11.  7. 11. 10. 12.]\n",
      " [ 7. 11.  0.  4.  8.  2.  4.]\n",
      " [ 9.  7.  4.  0.  4.  3.  5.]\n",
      " [13. 11.  8.  4.  0.  7.  9.]\n",
      " [ 9. 10.  2.  3.  7.  0.  2.]\n",
      " [11. 12.  4.  5.  9.  2.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Reference distance matrix from Floyd-Warshall. \n",
    "test[test == 0] = np.inf\n",
    "np.fill_diagonal(test, 0)\n",
    "FW_dist_matrix, predecessors = shortest_path(csgraph=test, directed=False, return_predecessors=True, method = 'FW')\n",
    "print(FW_dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67b4de16-3158-4dbf-a759-bc0831e3ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_plus_1(A, B):\n",
    "    B = np.transpose(B)\n",
    "    Y = np.zeros((len(B),len(A)))\n",
    "    for i in range(len(B)):\n",
    "         Y[i] = (A + B[i]).min(1)\n",
    "    return np.transpose(Y)\n",
    "\n",
    "test[test == 0] = np.inf\n",
    "np.fill_diagonal(test, 0)\n",
    "out = test\n",
    "\n",
    "#%timeit min_plus_1(test, test)\n",
    "#min_plus_1(test, test)\n",
    "for i in range(len(test)):\n",
    "    out = min_plus_1(out, test)\n",
    "\n",
    "np.testing.assert_array_equal(out, FW_dist_matrix) # if successful, returns nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a9a4f91-dbae-4de9-9bb1-42854041b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247 µs ± 2.45 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "out = test\n",
    "for i in range(len(test)):\n",
    "    out = min_plus_1(out, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "818006f2-716c-43bd-b216-e0ebe35a5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_plus_2(A, B):\n",
    "    return (np.expand_dims(A, 0) + np.expand_dims(B.T, 1)).min(axis=2).T\n",
    "\n",
    "test[test == 0] = np.inf\n",
    "np.fill_diagonal(test, 0)\n",
    "out = test\n",
    "\n",
    "#%timeit min_plus_2(test, test)\n",
    "#min_plus_2(test, test)\n",
    "for i in range(len(test)):\n",
    "    out = min_plus_2(out, test)\n",
    "    \n",
    "np.testing.assert_array_equal(out, FW_dist_matrix) # if successful, returns nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a36f4dda-3911-441d-b87d-ec6fabafb45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 µs ± 766 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "out = test\n",
    "for i in range(len(test)):\n",
    "    out = min_plus_2(out, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22957015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_plus_3(A, B):\n",
    "    A = A[:, :, np.newaxis] # shape (a, b, 1)\n",
    "    B = B[np.newaxis, :, :] # shape(1, b, c)\n",
    "    \n",
    "    # A+B will have shape (a, b, c)\n",
    "    return (A + B).min(axis=1) \n",
    "\n",
    "test[test == 0] = np.inf\n",
    "np.fill_diagonal(test, 0)\n",
    "out = test\n",
    "\n",
    "#%timeit min_plus_3(test, test)\n",
    "#min_plus_3(test, test)\n",
    "for i in range(len(test)):\n",
    "    out = min_plus_3(out, test)\n",
    "    \n",
    "np.testing.assert_array_equal(out, FW_dist_matrix) # if successful, returns nothing   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe813866-7e85-4bea-9bd7-d1b15605c79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.4 µs ± 1.72 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "out = test\n",
    "for i in range(len(test)):\n",
    "    out = min_plus_3(out, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c99a82",
   "metadata": {},
   "source": [
    "# Appendix: Practical Performance / Compilers\n",
    "\n",
    "You may be wondering how the distance products I calculated are (on my end) running faster than the scipy implementation of FW .  \n",
    "For that matter, you may wonder why FW is faster than any of the single-pair algorithms.  \n",
    "I would first note that I have only shown the time for fixed input, on a relatively small (dense) graph.  \n",
    "This is both for clarity of explanation, and because I want you to check the time complexities of your own voilition.  \n",
    "If you start changing the input (either the number of vertices or the relative number of edges), there is no guarantee these rankings will stay the same.  \n",
    "\n",
    "Aside from that, I will also mention that scipy's Dijkstra uses Fibonacci heaps because they give the best *theoretical* performance.  \n",
    "The *actual* performance ultimately depends on many things including cache locality and compiler optimizations.  \n",
    "So the implementation may be the best for asymptotically large graphs.  \n",
    "But we never said how large you would have to go for that to actually matter.  \n",
    "In the end, you should test your work for a given system and problem.  \n",
    "To give you a taste of this, I will mention one other concept: compilers.  \n",
    "\n",
    "Recall that Python is a high-level language that is meant to be interpretable to humans.  \n",
    "In order to have the computer execute it, the high level code must be translated to machine-readable code.  \n",
    "For Python in most operations, it actually goes through a few translations (bytecode, PVM, C, etc.) before reaching that state.   \n",
    "If we really want to speed up operations, we need to get to low-level instructions as fast as possible.   \n",
    "\n",
    "The most direct way would be to learn lower-level languages (C++, Fortran, assembly, etc.), but clearly I'm not going to do that right now.  \n",
    "Instead, we want to use a compiler that takes Python code and compiles more directly to machine-readable code.  \n",
    "This is where things like [numba](https://numba.pydata.org/) and [JAX](https://docs.jax.dev/en/latest/quickstart.html) come in.  \n",
    "They do exactly what we want: take at least some Python and numpy code, and use a (JIT) compiler to speed up machine translation and optimize execution. \n",
    "**Sidenote:** In case you were curuious, numba does its compilation through [LLVM](https://en.wikipedia.org/wiki/LLVM).\n",
    "\n",
    "Below I put some demo code for a min-plus product calculator, and its numba-enahanced version.  \n",
    "The base function uses a triply-nested `for`-loop to run element-by-element, so it is incredibly slow.  \n",
    "But we need to cast it into this form because numba does not accept many of the functions of numpy that we used above, specifically the `min` portions.  \n",
    "To use it with numba, I put the `@njit` decorator on the function.  \n",
    "\n",
    "The times will depend on the system/inputs, but by default on my system(s), numba should have a factor of $\\approx$ 100 speedup.  \n",
    "In fact, the numba-enhanced version of (somewhat inelegant) code even beats the best broadcast-based version above by a factor of $\\approx$ 5.  \n",
    "Again, this does not account for the time complexity so the results are not generalizeable to larger graphs.  \n",
    "Still, that is a huge improvement and we should pay more attention to opportunities like this.  \n",
    "**Moral of the Story:** Learn low-level languages; failing that, for high-level languages, check if a more direct compiler exists/helps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1a48e3d-d9d5-4ae3-a068-831fe03c4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_plus_4(A, B):\n",
    "    n, m = A.shape\n",
    "    m2, p = B.shape\n",
    "\n",
    "    C = np.full((n, p), np.inf)\n",
    "    for i in range(n):\n",
    "        for j in range(p):\n",
    "            for k in range(m):\n",
    "                temp = A[i, k] + B[k, j]\n",
    "                if temp < C[i, j]:\n",
    "                    C[i, j] = temp\n",
    "    return C\n",
    "\n",
    "test[test == 0] = np.inf\n",
    "np.fill_diagonal(test, 0)\n",
    "out = test\n",
    "\n",
    "#%timeit min_plus_4(test, test)\n",
    "#min_plus_4(test, test)\n",
    "for i in range(len(test)):\n",
    "    out = min_plus_1(out, test)\n",
    "\n",
    "np.testing.assert_array_equal(out, FW_dist_matrix) # if successful, returns nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45674af2-a252-40ad-aabd-abd58923c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 ms ± 12.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "out = test\n",
    "for i in range(len(test)):\n",
    "    out = min_plus_4(out, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2997713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def njit_min_plus_4(A, B):\n",
    "    n, m = A.shape\n",
    "    m2, p = B.shape\n",
    "\n",
    "    C = np.full((n, p), np.inf)\n",
    "    for i in range(n):\n",
    "        for j in range(p):\n",
    "            for k in range(m):\n",
    "                temp = A[i, k] + B[k, j]\n",
    "                if temp < C[i, j]:\n",
    "                    C[i, j] = temp\n",
    "    return C\n",
    "\n",
    "test[test == 0] = np.inf\n",
    "np.fill_diagonal(test, 0)\n",
    "out = test\n",
    "\n",
    "#%timeit njit_min_plus_4(test, test)\n",
    "#njit_min_plus_4(test, test)\n",
    "for i in range(len(test)):\n",
    "    out = njit_min_plus_4(out, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fa202a6-35da-4d17-a968-f7283606890d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4 µs ± 1.62 µs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "out = test\n",
    "for i in range(len(test)):\n",
    "    out = njit_min_plus_4(out, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fddb07-38a2-450b-a1a9-302f6687a751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
